# LRB

**Learning Relaxed Belady for Content  Distribution Network Caching**

<br>

[출처] NSDI’20: 17th USENIX Symposium on Networked Systems Design and Implementation

<br>

> CDN
>
> Cache replacement algorithm
>
> Belady's algorithm

<br>

## 1. CDN

>  **Content Distribution Networks (CDNs)** deliver content through a network of caching servers to users to improve latency, bandwidth, and availability.
>
>  A CDN cache sits between **users** and **Wide-Area Networks (WANs)**. When the user requests content, the CDN server provides cached content.

<br>

  Whenever a user requests content that is not currently cached, the CDN must fetch this content across the **Internet Service Provider (ISPs)** and WANS. While CDNs are paid for the bytes delivered to users, they need to pay for the bandwidth required to fetch cache misses. These bandwidth costs constitute an increasingly large factor in the operational cost of large CDNs. CDNs are thus **seeking to minimize these costs**, which are typically measured as byte miss ratios, i.e., the fraction of bytes requested by users that are not served from cache.

<br>

  **The algorithm that decides which objects are cached** in each CDN server plays a key role in achieving a low byte miss ratio. Yet, the state-of-the-art algorithms used in CDN caching servers are heuristic-based variants of the **Least Recently Used (LRU)** algorithm. The drawback of heuristics-based algorithms is that they typically work well for some access patterns and poorly for others. And, still observe a large gap between the byte miss ratios of the state-of-the-art online cache replacement algorithms and **Belady's offline MIN algorithm** on a range of production traces.

<br>

## 2. LRB

  To bridge gap between LRU and Belady's algorithm, this paper presents a novel machine learning (ML) approach that is fundamentally different from previous approaches to cache replacement. Key approach is to **approximate Belady's MIN (oracle) algorithm, using machine learning to find objects to evict** based on past access patterns. 

<br>

  Belady's algorithm always evicts the object with the furthest next request. A **naive ML algorithm that imitates this behavior** would incur prohibitively **high computational cost**. To overcome this challenge, our key insight is that it is sufficient to approximate a **relaxed Belady algorithm** that evicts an object whose next request is **beyond a threshold** but not necessarily the farthest in the future.

<br>

It allows our system to **run predictions on a small sampled candidate set**. e.g., 64, which dramatically reduces computational cost. And, it allows our system to **quickly adapt to changes** in the workload by gathering training data that includes the critical examples of objects that relaxed Belady would have selected for replacement.

<br>

**Even with the insight of relaxed Belady, the design space** of potential ML architectures (features, model, loss function, etc.) is **enormous**. **Exploring this space** using end-to-end metrics like byte miss ratio is prohibitively time-consuming because they require full simulations that **take several hours** for a single configuration. 

<br>

To enable our exploration of the ML design space this paper introduces an eviction decision quality metric, the **good decision ratio**, that evaluates if the next request of an evicted object is beyond the Belady boundary. The good decision ratio allows us to **run a simulation only once** to gather training data, prediction data, and learn the Belady boundary. Then we can use it to quickly evaluate the quality of decisions made by an ML architecture **without running a full simulation**. We use the good decision ratio to explore the design space for components of our ML architecture including its features, model, prediction target, and loss function among others.

<br>

These concepts enable to design the Learning Relaxed Belady (LRB) cache, the first CDN cache that approximates Belady’s algorithm.

<br>

## 3. Belady's Algorithm

In order to approximate Belady’s MIN algorithm in the design of an ML-based cache, this paper introduces the relaxed Belady algorithm, Belady boundary, and good decision ratio.

<br>

### Relaxed Belady Algorithm

  It is difficult for an ML predictor to directly approximate Belady’s MIN algorithm for two reasons.

- The **cost** of running predictions for all objects in the cache can be prohibitively **high**.
- In order to find the object with the farthest next request, an ML predictor needs to **predict the time to next request of all objects in the cache accurately**.



![1](https://user-images.githubusercontent.com/56579239/202909056-c1fd90e5-47b4-4a96-a9b9-1e40d50f35cd.png)

*Figure 1: The relaxed Belady algorithm partitions objects into two sets based on their next request. The next requests of are beyond the threshold (Belady boundary).*

<br>

  To overcome these two challenges, we define the relaxed Belady algorithm, a variant of Belady’s MIN, that randomly **evicts an object whose next request is beyond a specific threshold**, as shown in Figure 1. The algorithm partitions the objects of a cache into two sets each time it needs to make an eviction decision.

- set 1: Objects whose next requests are **within the threshold**.
- set 2: Objects whose next requests are **beyond the threshold**.

If set 2 is not empty, the algorithm randomly evicts an object from this set. **If set 2 is empty**, the algorithm **reverts to the classical Belady** among object in set 1.

<br>

  The relaxed Belady algorithm has two important properties.

- If the **threshold is far**, its byte miss ratio will be **close to that of Belady’s MIN**.
-  since the algorithm can evict any of the objects in set 2, not necessarily the object with farthest distance, **there will be many eviction choices** in the cache each time it needs to evict an object.

<br>

  These properties greatly reduce the requirements for an ML predictor. **Instead of predicting next requests for all objects** in the cache, the ML predictor **can run predictions on a small sample of candidates** in the cache for an eviction as long as the sample includes objects from set 2. This allows a dramatic reduction of computational cost. In addition, it reduces the required precision of the ML predictor. It only needs to find an object whose next request is beyond the threshold instead of the farthest. This greatly **reduces the memory overhead** when gathering training data: instead of having to track objects indefinitely, it is sufficient to track objects until they are re-requested or cross the threshold. 

<br>

  Shorter thresholds thus make the task of the ML predictor more tractable. While longer thresholds move the byte miss ratio of relaxed Belady closer to Belady’s MIN. It is thus **important to find the proper threshold**.

<br>

### Belady Boundary

  To **systematically choose a threshold** for the relaxed Belady algorithm, we introduce the **Belady boundary as the minimum of time to next request for all evicted objects** by Belady’s MIN algorithm. It is intuitively meaningful, Belady’s MIN kept all objects with a next request less than this boundary in the cache, but requires future information to compute.

<br>

  In practice, we assume that the Belady boundary is approximately stationary. This allows us to approximate the Belady boundary by computing the minimum of the next requests of all evicted objects by the Belady MIN algorithm during the machine learning warmup period. 

<br>

![2](https://user-images.githubusercontent.com/56579239/202909058-b101b7df-29e4-4981-a9d5-1f774f227957.png)

*Table 1: Comparison of the relaxed Belady algorithm to Belady MIN for the Wikipedia trace with different cache sizes.*

<br>

  Table 1 shows how the Belady boundary affects the byte miss ratio of relaxed Belady on the Wikipedia trace. We find that relaxed Belady applies random eviction to a set of objects, e.g., 19% for a 64 GB cache. While this Belady boundary enables efficient implementations, it comes at the cost of 9-13% more misses. Compared to the 25%–40% gap between stateof-the-art online algorithms and Belady’s MIN, this increase seems acceptable. 

<br>

  Next, show how to use the Belady boundary to establish a decision quality metric, which is used to make design decisions.

<br>

### Good Decision Ratio

To design an algorithm that makes better decisions we need to determine the quality of individual decisions. End-to-end metrics like byte miss ratio, however, reflect the aggregated quality of many decisions. When an algorithm has a byte miss ratio higher than Belady we know it made some bad decisions, but we cannot determine which of its decisions were bad. The individual misses that comprise byte miss ratio are similarly unhelpful because they are also the result of many earlier decisions.

<br>

 Our eviction decision metric is defined with respect to the relaxed Belady algorithm with the Belady boundary as its threshold. Evicting an object is a good decision if the next request of the object is beyond the Belady boundary. It is a bad decision if its next request is within the Belady boundary. 

<br>

  We find that an algorithm’s good decision ratio — i.e., # good decisions / # total eviction decisions — correlates strongly with the byte miss ratio it ultimately achieves. This metric plays a key part in the design of LRB.

<br>

## 4. Design

This section presents the design details of LRB, which uses ML to imitate the relaxed Belady algorithm. Accomplishing this requires simultaneously addressing two previously unsolved problems: 

- How to design an ML approach that decreases byte miss ratio relative to state-of-the-art heuristics? 
- How to build a practical system with that approach?

<br>

  Each problem introduces several challenges in isolation, simultaneously addressing them additionally requires us to balance their often-competing goals. 

<br>

![3](https://user-images.githubusercontent.com/56579239/202909059-639db6b0-a2ac-4872-8cc6-10aa3b5f93d0.png)

*Figure 2: General architecture that uses ML for caching.*

<br>

  Figure 2 presents a general architecture that uses ML to make eviction decisions. This paper identifies four key design issues:

**(1) Past information**: The first design issue is determining how much and what past information to use. More data improve training quality, but we need to limit the information in order to build a practical system, as higher memory overhead leads to less memory that can be used to cache objects. 

**(2) Training data**: The second design issue is how to use past information for training. As workloads vary over time the model must be periodically retrained on new data. Thus, a practical system must be able to dynamically generate training datasets with proper labels. 

**(3) ML architecture**: The third design issue is selecting a machine learning architecture that makes good decisions. This includes selecting features, the prediction target, and the loss function as well as how to translate predictions into eviction decisions. In addition, these decisions need to be compatible with available hardware resources to build a practical system.

 **(4) Eviction candidate selection**: The final design issue is how to select eviction candidates. Although an approximation of the relaxed Belady algorithm can evict any object whose next request is beyond the Belady boundary and there are many eviction choices, we still need a method to select a small set of candidates that includes such objects with a high probability. 

<br>

### Past information

<br>

### Training data

<br>

### ML Architecture

<br>

### Eviction Candidate Selection

<br>

### Putting all Together

<br>

## 5. Conclusion

  The key advantage of using ML to approximate Belady’s MIN algorithm over access-pattern specific or heuristicsbased approaches is that it can intelligently make cache eviction decisions based on any access pattern.

<br>

  More importantly, This paper has introduced the relaxed Belady algorithm, Belady boundary, and good decision ratio as an eviction quality metric, which has enabled to take a fundamentally new approach to caching that approximates Belady’s MIN algorithm. These concepts can benefit others in the future.

<br>

  LRB’s implementation is practical and deployable by replacing the caching component of a production CDN caching system with LRB. Our experiments show that its throughput and latency are on par with the native implementation. LRB requires neither GPUs nor accelerators and, in fact, its additional CPU and memory overheads are moderate and within the constraints of today’s CDN server hardware. This deployable design is enabled by key design decisions including our feature selection, sliding memory window, training data selection, and choice of a lightweight machine learning model.





