---
title:  "FlexTOE: Flexible TCP Offload"
date:   2022-07-11 17:23:03 +0900
categories: TOE SmartNIC
author_profile: true
toc: true
toc_sticky: true
toc_label: "FlexTOE"
---
FlexTOE: Flexible TCP Offload with Fine-Grained Parallelism

[출처] NSDI'22:Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation, April 4–6, 2022 • Renton, WA, USA

## 1.	배경
TCP는 데이터 센터에 기본 프로토콜로 사용되고 있다.
네트워크 속도가 CPU 속도를 넘어섬에 따라 TCP의 CPU 오버헤드가 중요해지고 있다.
다음은 다른 TCP 프로세싱 접근 방법들이 CPU 성능에 영향을 미치는 요소들을 표로 나타낸 것이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/178402679-7f0ca0be-c1ca-47b0-bc1c-4dbd9dc6d632.png">
</p>

- **In-kernel (Linux)**

Linux의 TCP 스택은 다용도로 사용된다. 하지만 큰 cache footprint와 비효율적인 실행, CPU 오버헤드라는 단점이 있다. 세그멘테이션과 같은 무상태 오프로드는 큰 전송의 경우 오버헤드를 감소한다. 하지만 짧은 flow로 이루어진 RPC의 워크로드에는 적은 영향을 보인다.

Linux는 위의 표에서 보다시피 매 요청마다 12.13kc 실행되는데, 이 중 10%만이 응용에 사용된다. 많은 명령과 Icache footprint, privilege mode 스위치, 흩어진 전역 변수, 잘 나누어지지 않은 locking은 명령어를 fetch하는 작업에만 62%의 사이클을 사용하게 한다. 

Linux는 원래 수정하기 쉬운 OS이지만, 커널 모드 개발의 경우 복잡하고 보안에 민감하다. 그래서 커널의 최적화나 새로운 네트워크 기능 개발이 있는 경우 종종 느려지기도 한다.

- **Inflexible TCP offload (Chelsio)**

Chelsio 같은 기존의 TCP offload 접근 방식은 offload를 하드웨어에 구현하였다. 이로 인해 데이터 센터 연산자는 TOE를 요구에 맞게 조정할 수 없게 되고 하드웨어 개발 주기가 길어져 업그레이드 속도가 느려진다. 예를 들어, Chelsio 터미네이터는 RPC 기반의 데이터 센터 워크로드에서 작업이 느려진다.

Chelsio의 융퉁성 없는 것은 위의 표에서도 나타난다. TCP 프로세싱 사이클이 매우 줄어들었음에도, 전체 요청 당 CPU 사이클은 Linux에 비해 27% 밖에 줄지 않았고, TAS에 비해서는 2.6배 많다. Chelsio 설계는 Linux 커널과의 상호작용을 필요로 하기 때문에, 실행 명령어는 50%가 줄었지만 비슷한 실행 결과를 보인다. 더하여 Chelsio는 복잡한 버퍼 관리와 동기화를 하는 매우 정교한 TOE NIC 드라이버를 필요로 한다.

따라서 Chelsio 설계는 RPC 프로세싱에 비효율적이며, 응용에는 전체 사이클의 16% 밖에 사용하지 않는다.

- **Kernel-bypass (TAS)**

kernel-bypass 방식은 TCP 스택을 응용으로 옮기는 방식으로 커널 오버헤드를 제거한다. TAS는 전용 코어에 protected user-mode TCP 스택을 실행하여 성능과 보안을 유지한다. TAS는 kernel 콜을 제거하여 socket API에 800 사이클만을 사용하고 있으며, Icache footprint와 명령어 fetch와 cache와 TLB 누락을 감소하면서 IPC를 증가시키고 전체 request 또한 감소하고 있다.

하지만 이런 kenel-bypass 방식 또한 상당한 오버헤드가 있는데, 이는 전체 사이클의 26%만이 응용에 사용되고 있다는 것이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/178405893-38e08273-9134-4dcd-bb0a-ceee4bd74e2d.png">
</p>

- **FlexTOE**

FlexTOE는 호스트의 TCP 스택 오버헤드를 전부 제거했다. 또한 요청 당 명령어 사이클 또한 TAS에 비해 2배 감소 했으며, 전체 사이클의 53%를 응용에 사용한다. 나머지 사이클은 TCP 오프로드에 제거될 수 없는 POSIX 소켓 API에 사용된다.

또한 FlexTOE는 융퉁성이 있어, operator가 TOE를 수정할 수 있도록 한다.

## 2. 소개
<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/179000065-2d562a4c-fa6c-40f0-83ec-2b45d9999f10.png">
</p>

SmartNIC은 융퉁성이 있으나 하드웨어 제한적이다. SmartNIC은 병렬적인 stateless offload에 초점이 맞춰져 있으며, 많은 wimpy core와 제한된 메모리로 이루여져 있다.
반면 TCP 연결은 순차적으로 이루어지는데, 이를 위해 stateful code를 통해 세그먼트를 분석한다. 또한 flow나 congestion control을 수행하며, 재전송에 민감하다.

이런 기존의 TCP 연결 방식은 SmartNIC 구조에서 성능이 매우 떨어진다. 그렇기 때문에 offload는 SmartNIC의 이용 가능한 모든 병렬성을 사용하여야 하며, 코어를 더욱 섬세하게 조절해야 한다. 

FlexTOE는 융퉁성에 더하여 다음 두 목표를 성취하려 한다.

- **Low tail latency and high throughput**

현대 데이터 센터 네트워크 작업은 짧은 flow와 긴 flow로 이루어져있다. RPC 같은 짧은 flow는 적은 지연 시간을 요구하고, 긴 flow의 작업은 높은 처리량을 요구한다. FlexTOE는 이 둘 다 제공하도록 한다.

- **Scalability**

서버에서 동시에 다뤄야 하는 네트워크의 flow나 응용의 문맥의 수가 증가하고 있다. FlexTOE는 이 요구에 따라 확장될 수 있어야 한다.

결론적으로 FlexTOE는 SmartNIC에게 융퉁성 있으면서 고성능인 TCP offload 엔진이다. 이를 위해, FlexTOE는 모듈 디자인을 통한 융퉁성을 유지하면서 wimpy SmartNIC 구조에서 고성능을 위해 TCP data-path의 섬세한 병렬화와 세그먼트의 재정렬을 활용한다.

## 3. 설계
위의 목표를 성취하고, SmartNIC 하드웨어의 제한을 극복하기 위해, FlexTOE는 다음 설계 원칙을 따른다.

**1. One-Shot data-path offload**

FlexTOE는 TCP의 RX(Receive)/TX(Transmit) data-path 오프로드에 집중하여 복잡한 제어, 계산, state를 제거한다. 이에 따라 섬세한 병렬화가 가능하게 한다. 거기에 FlexTOE의 data-path offload는 TCP의 각 세그먼트마다 한 번에 이뤄진다. 세그먼트는 NIC에 버퍼되지 않으며 SmartNIC의 메모리 관리를 매우 단순화 한다.

**2. Modularity**

FlexTOE는 TCP data-path를 섬세하게 커스터마이징하여 state는 계속 private하며, 명시적으로 상호작용하는 모듈로 분해한다. 그리고 TCP extansion을 모듈로 구현하고 데이터 흐름에 연결함으로써 개발 및 통합을 단순화할 수 있다.

**3. Fine-grained parallelism**

FlexTOE는 data-path 모듈을 SmartNIC 자원을 최대한 활용할 수 있는 데이터 병렬 계산 파이프라인으로 구성한다. 그리고 단계를 FPC에 맞춰, FPC 자원을 충분히 사용할 수 있게 한다. 또한, TCP 세그먼트 순서 지정 및 재정렬을 사용하여 병렬성 및 파이프라인 단계의 비정렬 처리를 지원하는 동시에 순차적 세그먼트 전송을 수행한다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/178477700-c6795a43-7908-4061-a26b-de2be368b4b1.png">
</p>

다음은 FlexTOE offload 구조이다. FlexTOE는 확장성을 위해 각 스레드 별로 context queue(`CTX-Qs`)가 있다. 각 TCP 소켓은 payload 버퍼(`PAYLOAD-BUFS`)를 호스트 메모리에 유지한다. libTOE는 소켓 당 `TX PAYLOAD-BUF`에 전송할 데이터를 저장한다. 그리고 data-path에 로컬 스레드의 `CTS-Qs`를 사용하여 알린다. data-path는 수신한 세그먼트를 재조합한 후 소켓의 `RX PAYLOAD-BUF`에 저장한다. 그리고 libTOE는 같은 로컬 스레드 `CTX-Q`를 이용하여 알린다. FlexTOE 트래픽이 아닌 경우에는 Linux 커널을 이용하여 알리며, 이 커널은 다른 레거시 응용과 함께 사용될 수 있다.

### 3.1. TCP Data-path Parallelization
SmartNIC의 wimpy FPCs를 이용하여 고성능의 offloading을 제공하기 위해, FlexTOE는 TCP data-path에서 이용 가능한 모든 병렬성을 이용해야 한다. 그러기 위해 TAS host TCP data-path를 세 작업 플로우에 맞춰 분석한다.

- **Host Control(HC)**: 응용이 데이터를 전송하려 할 때 소켓의 연산을 실행하고, 재전송이 필요할 때 data-path는 송수신 윈도우를 업데이트 해한다.
- **Transmit(TX)**: TCP 연결이 전송을 준비할 때, data-path는 소켓 전송 버퍼에서 payload를 fetch 하고, MAC으로 전송하면서 전송에 필요한 세그먼트를 준비한다. 
- **Receive(RX)**: 만들어진 연결에서 수신된 세그먼트를 data-path는 바이트 스트림으로 재조립 후 수행한다. TCP 윈도우를 전진시키고, 소켓 수신 버퍼에 세그먼트 위치를 결정하고, 송신자에게 보낼 ack 응답을 생성하고, 응용에 알린다. 만약 수신된 세그먼트가 이전에 전송했던 세그먼트이면, data-path는 소켓 전송 버퍼의 payload를 할당 해제 한다.

Host TCP 스택은 공유 되는 각 연결의 상태 구조에 접근하는 임계영역에서 각 작업을 완료하려 할 것이다. HC는 응용 스레드에 의해 트리거 되고, TX/RX는 NIC 인터럽트에 의해 트리거 되며, 커널이나 전용 스레드 같은 높은 우선 순위의 스레드에서 수행된다.


<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/178986920-ad22645d-8ae9-42e3-b814-25c70c4b473a.png">
</p>

효율적인 offload를 위해, data-path를 5단계의 병행적 파이프라인으로 분해한다: `pre-processing`, `protocol`, `post-processing`, `DMA`, `context queue`. 따라서 연결 상태를 모듈 로컬 상태로 나눈다. 파이프라인 단계는 data-path의 병행성을 최대화 할 수 있도록 선택한다.

- **pre-processing**: 세그먼트 헤더를 준비하고 필터링하기 위해 MAC이나 IP 주소 같은 연결된 식별자에 접근한다. 
- **post-processing**: 소켓 버퍼 주소나 context queue 같은 응용 인터페이스 파라미터를 처리한다. 파라미터는 연결이 만들어진 후 읽어지며, 협동-free 조절이 가능하다. 혼잡 제어 통계는 이 단계를 통해 수집되며, 앞의 단계만 읽을 수 있고, 단계가 다 끝나야 업데이트 될 수 있다.
- **protocol**: 순서 번호나 소켓 버퍼 위치 같은 프로토콜 상태를 원자적으로 수정하는 data-path 코드를 실행한다.
- **DMA & CTX-Q**: DMA 단계는 context queue 단계에서 쪼개진 것이므로 stateless이다. 둘 다 PCIe 트랜잭션의 높은 지연시간을 수행하기 때문에 병렬적으로 수행할 수 있도록 나뉘었다.

파이프 라인 단계는 연결 상태의 부분을 위해 로컬 메모리를 사용할 수 있도록 전용 FPCs 코어에서 실행된다. 파이프라인으로 나눈 것은 data-path가 병렬적으로 실행될 수 있도록 한다. 추가적인 FPCs를 조정하기 위해 프로세싱 집약적인 파이프라인 단계를 복사할 수 있도록 한다. 각 연결마다 원자적인 프로토콜 프로세싱 단계를 제외하고, 모든 파이프라인 단계는 복사된다. 여러 연결을 동시에 실행하기 위해, 파이프라인 전체를 복사할 수도 있다. 흐름 상태를 로컬로 유지하기 위해 각 파이프라인은 흐름 4-튜플의 해쉬에 의해 결정된 고정된 흐름그룹을 다룬다.

<h4> 1. HC </h4>

<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/178989583-586fcbcf-4c18-4cbf-b20c-1e3fac8de6c8.png">
</p>

HC는 host의 context queue 단계에서 MMIO를 통해 전달되는 PCIe DB에 의해 트리거된다. 다음은 libTOE에 의해 트리거 된 두 전송의 HC 파이프라인과 control-plane에 의해 재전송되는 트리거를 보여준다. HC 요청은 수행될 것이다.

Context queue 단계는 DB를 기다릴 것이다. DB에 응하여, Context queue 단계는 적은 NIC 메모리로부터 디스크립터 버퍼를 할당할 것이다. 제한된 사이즈의 흐름은 host 상호작용을 제어할 것이다. 할당에 실패하면, 프로세싱을 멈춘다. 반면에 DMA 단계는 host context queue로부터 버퍼에 디스크립터를 fetch한다. pre-processor는 디스크립터를 읽고, 흐름 집단을 결정하고 적절한 프로토콜 단계로 라우트한다. 프로토콜 단계는 연결된 송수신 윈도우를 업데이트 한다. HC 디스크립터가 닫힌 연결을 포함하면, 프로토콜 단계는 연결에 FIN 마크를 한다. 응용이 전송을 위해 데이터를 보내면 전송 윈도우가 확장된다. post-processor는 흐름 스케줄러를 업데이트 하고 디스크립터를 pool에 반환한다.

타임아웃에 응하여 재전송은 control-plane에 의해 트리거 되고, 다른 HC 이벤트와 같이 진행된다. 프로토콜 단계는 마지막 ACKed 순서 넘버로 전송 state를 초기화한다. (고백 N 방식)

<h4> 2. TX </h4>
<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/179008768-40301bf6-a1c6-4e19-a7b0-b51a82b498a7.png">
</p>

전송은 연결이 세그먼트를 전송할 수 있을 때, 흐름 스케줄러에 의해 트리거 된다. 다음은 전송 파이프라인의 3가지 예이다.

pre-processor는 세그먼트를 NIC 메모리에 할당한다(Alloc). 그리고 이더넷과 IP 헤더를 준비하고(Head), 흐름 그룹의 프로토콜 단계에 세그먼트를 조종한다(steer). 프토콜 단계는 연결 state에 따라 TCP 순서 번호를 결정하고, 호스트 소켓 전송 버퍼에 전송 오프셋을 결정한다(Seq). post-processor는 호스트 메모리의 소켓 전송 버퍼 주소를 결정한다(Pos). DMA 단계는 호스트의 payload를 세그먼트로 fetch한다(payload). DMA가 완료되면, 세그먼트를 전송하고 할당 해제하는 NBI에 세그먼트를 보낸다(TX).

<h4> 3. RX </h4>
<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/179013966-d67215c9-e533-46c4-9387-26c2e0628718.png">
</p>

- **pre-processing**: pre-prosessor는 처음으로 세그먼트 헤더를 검증한다(Val). Non-data-path 세그먼트는 필터링 되고, control-plane으로 전송된다. 반면에 pre-processor는 세그먼트 4-tuple를 기반으로 연결 상태에 접근하는 나중 단계에서 사용할 연결 번호를 결정한다(Id). pro-processor는 나중의 파이프라인 단계에 요구되는 헤더 필드를 포함하는 헤더 요약을 생성하고(Sum), 흐름 그룹의 프로토콜 단계에게 연결 식별자를 생성한다(Steer).
- **protocol**: 헤더 요약을 기반으로 프로토콜 단계는 연결의 시퀀스 및 확인 번호, 전송 윈도우를 업데이트하고 호스트 소켓 수신 페이로드 버퍼에서 세그먼트의 위치를 결정하여 필요한 경우 수신 윈도우에 맞도록 페이로드를 트리밍한다(Win). 프로토콜 단계는 또한 중복 ACK를 추적하고 필요한 경우 전송 상태를 마지막으로 확인된 위치로 재설정하여 빠른 재전송을 트리거합니다. 마지막으로, 관련 연결 상태의 스냅샷을 후 처리로 전달합니다.

    순서가 잘못된 도착(그림 6의 세그먼트 3)은 특별한 처리가 필요합니다. TAS[19]와 마찬가지로, 우리는 수신 창에서 하나의 고장 간격을 추적하여 프로토콜 단계가 호스트 소켓 수신 버퍼 내에서 직접 재조립을 수행할 수 있도록 한다. 호스트 수신 버퍼의 간격 내에 순서가 잘못된 세그먼트를 병합합니다. 간격 밖의 세그먼트는 삭제되고 예상 시퀀스 번호로 확인 응답을 생성하여 송신자에서 재전송을 트리거합니다. 이 설계는 손실 하에서 잘 수행된다(제5.3항 참조).

- **post-processing**: post-processor는 ack 세그먼트를 준비한다(Ack). FlexTOE는 명시적인 혼잡 알림 (ECN) 피드백을 제공하고, RTT 측정을 위한 정확한 타임 스탬프를 제공한다(Stamp). 또한 혼잡 제어 및 전송 윈도우 통계를 수집하여, control-plane 및 흐름 스케줄러로 전송한다(Stats). 마지막으로 호스트 소켓 수신 버퍼의 물리적 주소, payload 오프셋 및 DMA 단계의 길이를 결정한다. libTOE가 통지되어야 하는 경우, post-processor는 적절한 알림과 함께 context queue 디스크립터를 할당한다.
- **DMA**: DMA 단계는 먼저 payload DMA 디스크립터를 PCIe 블록에 큐잉한다(Payload). payload DMA가 완료된 후, DMA 단계는 알림 디스크립터를 context queue 단계로 전달한다. 동시에, 준비된 ack 세그먼트를 전송 후 세그먼트 할당 해제하는 NBI로 전송한다(TX). 이 순서는 호스트 소켓 수신 버퍼로 데이터 전송이 완료되기 전에 호스트와 피어가 알림을 수신하지 못하도록 하는 데 필요합니다.
- **context queue**: 필요한 경우, context queue 단계는 context queue에 엔트리를 할당하고, 새 payload의 libTOE에 알리기 위해 context queue 디스크립터 DMA를 만든다(Notify). 그리고 내부 디스크립터 버퍼를 해제한다(Free).

### 3.2. Sequencing and Reordering
TCP는 같은 연결의 세그먼트가 receiver가 loss detection 할 수 있도록 순서대로 수행된다. 그러나 FlexTOE의 병렬적인 파이프라인 수행 단계는 수행 시간이 다양하다. 그래서 수신한 세그먼트를 재정렬해야 할 수 있다. 다음은 재정렬이 필요한 세 가지 예제이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/56579239/179015975-9437bb04-5d5f-4804-8b61-f4ee9b341e46.png">
</p>

- **TX**: TX 세그먼트 #1이 정체된 PCIe 링크를 통해 DMA에서 정지되어 TX 세그먼트 #2 이후 네트워크에서 전송되어 수신자 손실 감지가 트리거될 수 있다.
- **RX**: RX 세그먼트 #1이 pre-processing 중에 흐름 식별에서 정지하여, RX 세그먼트 #2보다 나중에 프로토콜 단계로 진입한다. 프로토콜 단계가 구멍을 감지하고 불필요한 순서 밖의 처리를 트리거합니다.
- **ACK**: TX 세그먼트 #3은 프로토콜 단계에서 RX 세그먼트 #1 이후에 처리된다. RX 세그먼트 #1은 ACK를 생성하지만, RX post-processing이 복잡하기 때문에 ACK 세그먼트 #1보다 시퀀스 번호가 높은 TX 세그먼트 #3이 전송된다.

재정렬을 방지하기 위해, FlexTOE의 data-path 파이프라인은 필요한 경우 세그먼트를 정렬하고 재정렬한다. 특히, 우리는 파이프라인으로 들어가는 각 세그먼트에 순서 번호를 할당한다. 병렬 파이프라인 단계는 각 세그먼트에서 임의의 순서로 작동할 수 있다. 프로토콜 단계는 순서대로 처리되어야 하며, 우리는 순서를 지키지 않고 도착한 세그먼트를 프로토콜 단계에 허용하기 전에 버퍼링하고 다시 정렬한다. 마찬가지로, NBI에 허용하기 전에 전송을 위해 세그먼트를 버퍼링하고 다시 정렬한다. 순서화, 버퍼링 및 재정렬을 위해 추가 FPC를 활용한다.

### 3.3. Flexibility
데이터 센터 네트워크는 빠르게 발전하고, TCP 스택은 연산자와 공급자에 의해 수정하기 쉬워지는 것을 요구한다. 많은 바람직한 데이터 센터 기능은 TOE 수정이 필요하며, 연산자에 의해 자주 조정 될 수 있는 것이다. FlexTOE는 프로그래밍 가능한 SmartNIC에 의해 TAS 같은 호스트 스택 위에서일지라도 이러한 특징을 구현하고 유지하는 데 필요한 유연성이 제공된다. TCP data-path의 개발 및 수정을 단순화하기 위해, FlexTOE는 확장 가능한 자체 포함 모듈의 데이터 병렬 파이프라인을 제공한다.

**Module API**
FlexTOE 모듈 API는 개발자에게 TCP 세그먼트와 메타 데이터에 한 번에 접근할 수 있도록 해준다. 메타데이터는 어떤 모듈에 의해 생성되고, 파이프라인을 따라 이동한다. 모듈은 private 한 상태를 유지한다. 확장성을 위해 private 상태는 다른 모듈이나 같은 모듈의 복사본에 의해 접근될 수 없으며, 대신 상태는 파이프라인 단계에 의해 접근되어 메타데이터에 전송될 수 있다.

FlexTOE에서 파이프라인 단계 및 FPC 할당의 복사 요소는 수동적이고 정적이다. 충분한 FPC를 사용할 수 있는 한, 이 접근 방식은 허용된다. 운영자는 배포 시 처리량 마이크로벤치를 통해 파이프라인 단계에서 허용되는 TCP 처리 대역폭을 산출하는 적절한 복사 요소를 결정할 수 있다. 연결 상태를 원자적으로 수정하는 단계는 원자 단계에서 모듈에 연결의 세그먼트를 조절하는 적절한 조절 단계를 삽입하여 그 상태를 유지함으로써 전개될 수 있다.

**XDP**
FlexTOE는 또한 eBPF로 작성된 XDP를 지원한다. XDP 모듈은 낮은 패킷에서 수행되며, 필요하면 수정 가능하다. 그리고 다음 결과 코드 중 하나를 출력한다.
1. `XDP_PASS`: 다음 FlexTOE 파이프라인 단계로 패킷을 전달한다.
2. `XDP_DROP`: 패킷을 drop 한다.
3. `XDP_TX`: MAC에 패킷을 전송한다.
4. `XDP_REDIRECT`: 패킷을 control-plane으로 리다이렉트 한다.

XDP 모듈은 state를 원자적으로 저장하고 수정하기 위해 control-plane에 의해 수정된 BPF map(arrays, hash tables)을 사용한다. 예를 들어, 방화멱 모듈은 hash map에 블랙리스트 IP를 저장하며 control-plane이 엔트리를 동적으로 지우거나 추가한다. 모듈은 해시 맵을 참조하여 패킷이 블랙리스트에 있는지 확인하고 삭제할 수 있다. XDP 단계는 모듈을 복사함으로써 다른 파이프라인 단계를 조정할 수 있다. FlexTOE는 병렬의 XDP 단계 후 수행한 세그먼트를 원자적으로 재정렬 할 수 있다.

이 API를 사용하여, FlexTOE의 data-path를 많이 수정하여 많은 기능을 구현했다. 게다가 ECN 피드백과 세그먼트 타임스탬프는 혼잡 제어 정책을 지원하는 TCP 선택적 기능이다. operator는 필요없으면 관련된 post-processing 모듈을 삭제할 수 있다. 

원자적, 병렬적, 순서대로 처리함으로써, FlexTOE는 복잡한 연산을 적은 줄의 코드를 사용하여 표현할 수 있도록 한다. 예를 들어, AccelTCP의 연결 분할을 24줄의 eBPF 코드로 구현했다. 모듈은 BPF 해시 맵에서 세그먼트 4-튜플을 검색한다. 맞는 것이 발견되지 않으면, 세그먼트를 다음 파이프라인 단계로 전송한다. 반면에, MAC 및 IP 주소, TCP 포트를 수정하고, 연결의 초기 순서 번호를 기반으로 control-plane에 의해 구현된 오프셋을 사용하여 순서 번호 및 ack 번호를 변환하여 전송한다. FlexTOE는 세그먼트의 체크섬 순서 지정 및 업데이트를 처리한다. 또한 연결이 닫힌 것을 나타내는 제어 플래그가 있는 세그먼트를 수신하면, 해시 맵 항목을 원자적으로 제거하고 control-plane에 알린다.

### 3.4. Flow Scheduling
FlexTOE는 NIC data-path에서 작업을 절약할 수 있는 흐름 스케줄러를 활용한다. 흐름 스케줄러는 control-plane의 혼잡 제어 정책에 의해 구성된 전송 속도 제한 및 윈도우을 따른다. 각 연결에 대해 플로우 스케줄러는 전송에 사용할 수 있는 데이터 양과 구성된 속도를 측정한다. 전송 속도 및 윈도우는 NIC 메모리에 저장되며 MMIO를 사용하여 control-plane에 의해 직접 업데이트 된다. 

FlexTOE는 Carousel을 기반으로 흐름 스케줄러를 구현한다. Carousel은 time wheel을 사용하여 많은 양의 흐름을 스케줄 한다. 속도 제한 및 윈도우에서 계산된 다음 전송 시간을 기준으로 time wheel의 해당 슬롯으로 흐름을 큐잉한다. 시간 슬롯 제한 시간이 경과하면, 흐름 스케줄러는 전송을 위해 슬롯의 각 흐름를 스케줄합니다. 작업을 절약하기 위해 흐름 스케줄러는 전송 윈도우가 0이 아닌 흐름만 time wheel에 추가하고 혼잡하지 않은 흐름에 대해서는 속도 제한기를 bypass 한다. 이런 흐름은 라운드 로빈에 의해 스케줄된다. 

## 4. Agilio-CS40 Implementation
5,801 lines of data-path in C(parts of data-path in assembly) + 4,620 lines of libTOE in C (from TAS) + 5,549 lines of control-path in C

NFP compiler toolchain version 6.1.0.1 for SmartNIC

- **Driver**
libTOE와 control plane이 유저 공간에서 MMIO를 SmartNIC으로 수행하기를 지원하는 igb_uio 드라이버를 기반으로 Linux FlexTOE 드라이버를 개발했다. 드라이버는 MSI-X 기반 인터럽트를 지원한다. control plane은 드라이버의 각 응용 context에 대한 `eventfd`를 등록한다. 드라이버의 인터럽트 핸들러는 응용 context의 data-path로부터 인터럽트가 수신될 때 해당 eventfd를 ping한다. 이것은 IO를 위해 기다릴 때, libTOE가 sleep하게 하고, polling을 위한 host CPU 오버헤드를 감소시킨다.

- **Host memory mapping**
DMA 연산을 위한 가상 주소를 물리 주소로의 변환을 단순화하기 위해, 1G 대용량 페이지를 사용하여 물리적으로 인접한 호스트 메모리를 할당한다. control plane은 시작할 때 1G의 거대한 페이지 풀을 매핑하고 소켓 버퍼와 컨텍스트 큐를 이 풀 밖으로 할당한다. 향후, IOMMU를 사용하여 FlexTOE 버퍼에 물리적으로 인접한 메모리가 필요하지 않게 할 수 있다.

- **Context queues**
Context queues은 호스트의 공유 메모리를 사용하지만 SmartNIC과 호스트 간의 통신에는 PCIe가 필요하다. NIC에서 실행할 때 NIC 내부 메모리나 호스트에서 실행할 때 호스트 메모리 위치를 폴링하는 확장 가능하고 효율적인 PCIe 통신 기술을 사용한다. NIC는 MMIO를 통해 NIC DB에 새 대기열 항목을 알려준다. context queue 관리자는 queue가 비활성화된 후, 드라이버에 의해 eventfd로 전환되는 MSI-X 인터럽트를 통해 응용에 알린다.

### 4.1. Near-memory Processing
NFP-4000의 다른 메모리 레벨의 접근 대기 시간에는 크기 차이가 존재한다. 성능을 위해서는 로컬 메모리에 대한 액세스를 최대화하는 것이 중요하다. NFP-4000은 또한 콘텐츠 주소 가능 메모리(CAM)를 노출하는 검색 엔진과 빠른 매칭을 위한 해시 테이블, 링크된 목록, 링 버퍼, 저널, 워크스틸링 큐와 같은 동시 데이터 구조를 노출하는 큐 메모리 엔진을 포함하여 특정 near-메모리 근처 가속을 제공한다. 마지막으로, 티켓 잠금 및 FPC 간 신호와 같은 동기화 프리미티브는 협응 스레드 및 순서 패킷에 노출된다. 이러한 프리미티브를 사용하여 서로 다른 파이프라인 단계에서 여러 수준에서 특수화된 캐시를 구축한다. 다른 NIC에는 유사한 가속 기능이 있다.

- **Caching**
각 FPC의 CAM을 사용하여 LRU를 기반으로 엔트리를 evict하는 16개 항목 완전 연결 로컬 메모리 캐시를 구축한다. 프로토콜 단계는 CLS에 512-엔트리가 직접 매핑된 2차 캐시를 추가한다. 4개의 island에 걸쳐, 최대 2K개의 흐름을 캐시에 수용할 수 있다. 최종 메모리 레벨은 EMEM에 있다. FPC는 세그먼트를 처리할 때, CLS 또는 EMEM에서 해당 상태를 로컬 메모리로 가져와 필요에 따라 다른 캐시 항목을 제거한다. 직접 매핑된 CLS 캐시에서 충돌을 최소화하는 방식으로 연결 식별자를 할당한다.

- **Active connection database**
전처리 단계에서 연결 인덱스 검색을 용이하게 하기 위해 IMEM의 하드웨어 검색 기능을 사용하여 활성 연결 데이터베이스를 유지한다. CAM은 해시 충돌을 해결하는 데 사용됩니다. 프리 프로세서는 세그먼트의 4-튜플에서 CRC-32 해시를 계산하여 룩업 엔진을 사용하여 연결 인덱스를 찾는다. 프리프로세서는 해시 값에 직접 매핑된 캐시를 통해 로컬 메모리에 최대 128개의 룩업 항목을 캐시한다.

- **FPC mapping**
FlexTOE의 파이프라인은 Agilio CX40을 완벽하게 활용하며 향후 FPC로 확장 가능합니다.
모듈 간의 섬-국소 상호 작용을 위해 CLS 링 버퍼를 사용한다. CLS는 가장 빠른 섬 내 생산자-소비자 메커니즘을 지원한다. 섬들 중에서, 우리는 IMEM과 EMEM의 작업 대기열에 의존한다.

    우리는 데이터 경로 파이프라인의 처음 세 단계(프로토콜 아일랜드)에 대해 하나의 범용 섬을 제외한 모든 섬을 사용한다. 각 섬은 흐름 그룹을 관리합니다. 프로토콜 및 후처리 FPC는 흐름 그룹에 로컬이지만, 사전 프로세서는 모든 흐름에 대한 세그먼트를 처리합니다. 우리는 각 흐름 그룹의 전/후 처리 단계에 4개의 FPC를 할당한다. 각 섬은 추가 데이터 경로 모듈(§ 5.1)을 실행할 수 있는 할당되지 않은 3개의 FPC를 보유하고 있다.

    나머지 범용 섬(서비스 섬이라고 함)에서는 컨텍스트 대기열 FPC, 흐름 스케줄러(SCH), DMA 관리자와 같은 나머지 파이프라인 단계 및 인접 모듈을 호스팅한다. DMA 관리자는 PCIe 대기 시간을 숨기기 위해 복제됩니다. 각 기능에 할당된 FPC의 수는 어떤 기능도 병목 현상이 되지 않도록 결정된다. 순서 지정 및 순서 변경 FPC는 기타 기능을 갖춘 다른 섬에 있습니다.

- **Flow scheduler**
우리는 EMEM의 하드웨어 대기열을 사용하여 Carousel을 구현한다. 각 슬롯에는 하드웨어 큐가 할당됩니다. 시간 휠에 흐름을 추가하기 위해 시간 슬롯과 연결된 큐에 흐름을 대기열에 넣습니다. 특정 슬롯 내의 흐름 순서는 유지되지 않습니다. 다수의 하드웨어 큐에 대한 EMEM 지원을 통해 슬롯 입도가 작고 수평선이 큰 타임휠을 효율적으로 구현하여 충실도가 높은 혼잡 제어를 달성할 수 있다. 전송 속도를 마감일로 변환하려면 분할이 필요하며 NFP-4000에서는 지원되지 않습니다. 따라서 제어 평면은 속도에서 주기/바이트 단위로 전송 간격을 계산하고 이를 NIC 메모리로 프로그래밍합니다. 이를 통해 흐름 스케줄러는 곱셈만 사용하여 시간 슬롯을 계산할 수 있습니다.

## 5. 결론
FlexTOE는 SmartNIC의 융퉁성 있지만 고성능인 TCP 오프로드 엔진이다. FlexTOE는 TCP data-path의 섬세한 병렬화를 활용하고, wimpy SmartNIC 구조의 고성능을 위해 모듈 설계를 통한 융퉁성을 유지하면서 세그먼트 재정렬을 활용한다. 

FlexTOE는 wimpy SmartNIC에서도 RPC에 대해서도 경쟁력 있는 고성능을 제공하며, 좋지 않은 OS에서도 견고하다. FlexTOE API는 eBPF로 작성된 XDP 프로그램도 지원한다. TCP 트레이싱이나 패킷 필터링, 캡쳐, VLAN 스티핑 등 인기 있는 데이터 센터 전송 기능을 구현할 수 있도록 한다.
